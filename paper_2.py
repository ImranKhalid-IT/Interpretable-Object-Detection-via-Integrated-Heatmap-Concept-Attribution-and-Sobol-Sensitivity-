# -*- coding: utf-8 -*-
"""paper 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18sNZ97lqv3IwXXDVRbiH2_AXblAQuAt6
"""

import cv2
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import models
import torchvision.transforms as T
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageDraw
import requests
from sklearn.decomposition import NMF
from typing import List, Dict, Optional, Tuple
import os
import warnings
import matplotlib.patches as patches
from matplotlib.colors import ListedColormap
import torch
import torch.nn.functional as F
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image
from typing import Optional, Dict

# --- Global COCO Name Lists (Unchanged) ---
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',
    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',
    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',
    'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]
DETR_CLASSES = [
    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
    'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
    'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv',
    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
    'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]



# --- DETRdemo CLASS DEFINITION (Unchanged) ---
class DETRdemo(nn.Module):
    def __init__(self, num_classes, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6):
        super().__init__()
        self.backbone = models.resnet50()
        del self.backbone.fc
        self.conv = nn.Conv2d(2048, hidden_dim, 1)
        self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers)
        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)
        self.linear_bbox = nn.Linear(hidden_dim, 4)
        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))
        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
    def forward(self, inputs):
        x = self.backbone.conv1(inputs); x = self.backbone.bn1(x); x = self.backbone.relu(x); x = self.backbone.maxpool(x)
        x = self.backbone.layer1(x); x = self.backbone.layer2(x); x = self.backbone.layer3(x)
        backbone_features = self.backbone.layer4(x)
        h = self.conv(backbone_features)
        H, W = h.shape[-2:]
        pos = torch.cat([self.col_embed[:W].unsqueeze(0).repeat(H,1,1), self.row_embed[:H].unsqueeze(1).repeat(1,W,1)], dim=-1).flatten(0,1).unsqueeze(1)
        h_transformed = self.transformer(pos + 0.1 * h.flatten(2).permute(2,0,1), self.query_pos.unsqueeze(1)).transpose(0,1)
        return {'pred_logits': self.linear_class(h_transformed),
                'pred_boxes': self.linear_bbox(h_transformed).sigmoid(),
                'conv_features': backbone_features,
                'img_tensor_fed_to_backbone': inputs}





# --- ODAM Processor (using DETR) (Unchanged) ---
class ODAM_Processor:
    def __init__(self, device='cuda'):
        self.device = device
        self.detr_model = DETRdemo(num_classes=91).to(self.device)
        try:
            sd = torch.hub.load_state_dict_from_url('https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth', map_location=self.device, check_hash=True)
            self.detr_model.load_state_dict(sd)
        except Exception as e: print(f"WARN: DETR weights load failed: {e}. Model will be uninitialized (random weights).")
        self.detr_model.eval()
        self.transform = T.Compose([T.Resize(800), T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])
        self.CLASSES = DETR_CLASSES
    def _box_cxcywh_to_xyxy(self,x): x_c,y_c,w,h=x.unbind(1); return torch.stack([(x_c-0.5*w),(y_c-0.5*h),(x_c+0.5*w),(y_c+0.5*h)],dim=1)
    def _rescale_bboxes(self,out_bbox_cxcywh,pil_size): img_w,img_h=pil_size; b=self._box_cxcywh_to_xyxy(out_bbox_cxcywh); return b*torch.tensor([img_w,img_h,img_w,img_h],dtype=torch.float32,device=out_bbox_cxcywh.device)

    def get_detections_features_and_input_tensor(self, pil_image, conf_thresh=0.7):
        img_tensor_transformed = self.transform(pil_image).unsqueeze(0).to(self.device)
        img_tensor_transformed.requires_grad_(True)
        with torch.set_grad_enabled(True):
            outputs = self.detr_model(img_tensor_transformed)
        pred_logits_batch = outputs['pred_logits']; pred_boxes_batch = outputs['pred_boxes']
        all_probas_queries = pred_logits_batch[0].softmax(-1)[:, :-1]
        keep_query_bool = all_probas_queries.max(-1).values > conf_thresh
        if not torch.any(keep_query_bool):
            print(f"ODAM Warning: No detections found above confidence {conf_thresh}")
            empty_features = outputs.get('conv_features', torch.empty(0,device=self.device)).detach()
            empty_input_tensor = img_tensor_transformed
            empty_proc_dims = tuple(empty_input_tensor.shape[-2:])
            return [], empty_features, outputs, empty_input_tensor, empty_proc_dims
        kept_raw_boxes_cxcywh_norm = pred_boxes_batch[0,keep_query_bool].detach()
        kept_probas_all_classes = all_probas_queries[keep_query_bool].detach()
        kept_raw_logits = pred_logits_batch[0,keep_query_bool]
        top_scores_for_kept,top_labels_idx_for_kept = kept_probas_all_classes.max(-1)
        bboxes_xyxy_orig_scale = self._rescale_bboxes(kept_raw_boxes_cxcywh_norm,pil_image.size)
        features_bchw = outputs['conv_features'].detach()
        final_detections_list = []
        kept_indices = keep_query_bool.nonzero(as_tuple=True)[0]

        for idx_in_keep, query_idx in enumerate(kept_indices):
                lbl_idx = top_labels_idx_for_kept[idx_in_keep].item()
                final_detections_list.append({ 'box_xyxy': bboxes_xyxy_orig_scale[idx_in_keep].cpu().detach().numpy(),
                                               'score': top_scores_for_kept[idx_in_keep].item(),
                                               'label_idx': lbl_idx,
                                               'class_name': self.CLASSES[lbl_idx] if lbl_idx < len(self.CLASSES) else "Unk",
                                               'raw_logits_for_det': kept_raw_logits[idx_in_keep],
                                               'query_index': query_idx.item()
    })

        proc_img_dims = tuple(img_tensor_transformed.shape[-2:])
        return final_detections_list,features_bchw,outputs,img_tensor_transformed,proc_img_dims

    def generate_gradient_explanation_heatmap(self, model_outputs: Dict, target_detection_dict: Dict,
                                            pil_image_original_size: Tuple[int,int]):
        conv_features = model_outputs['conv_features']
        if conv_features.grad is not None: conv_features.grad.zero_()
        score_to_backprop = target_detection_dict['raw_logits_for_det'][target_detection_dict['label_idx']]
        if not conv_features.requires_grad:
            warnings.warn("Grad-CAM: conv_features does not initially require grad. This may cause an error or an empty heatmap.")

        score_to_backprop.backward(retain_graph=True)
        if conv_features.grad is None:
            warnings.warn("Grad-CAM: No gradients on conv_features. Returning zero map."); return torch.zeros(pil_image_original_size[1],pil_image_original_size[0],device=self.device).cpu()
        pooled_gradients = torch.mean(conv_features.grad,dim=[0,2,3])
        heatmap_feat_sized = torch.zeros_like(conv_features[0,0,:,:]).float()
        for k in range(conv_features.shape[1]): heatmap_feat_sized += pooled_gradients[k]*conv_features[0,k,:,:]
        heatmap_feat_sized = F.relu(heatmap_feat_sized)
        hm_min,hm_max=heatmap_feat_sized.min(),heatmap_feat_sized.max()
        if hm_max-hm_min > 1e-8: heatmap_feat_sized=(heatmap_feat_sized-hm_min)/(hm_max-hm_min)
        else: heatmap_feat_sized=torch.zeros_like(heatmap_feat_sized)
        heatmap_orig_scale = F.interpolate(heatmap_feat_sized.unsqueeze(0).unsqueeze(0),size=(pil_image_original_size[1],pil_image_original_size[0]),mode='bilinear',align_corners=False).squeeze()
        return heatmap_orig_scale.cpu().detach()



# --- SobolAnalyzer CLASS (Unchanged) ---
class SobolAnalyzer:
    def __init__(self, eps: float = 1e-8): self.eps = eps
    def _box_to_mask(self, box: torch.Tensor, H_f: int, W_f: int, H_i: int, W_i: int) -> torch.Tensor:
        m=torch.zeros(H_f,W_f,dtype=torch.float32,device=box.device); x1,y1,x2,y2=box.int()
        x1f,y1f,x2f,y2f=int((x1/float(W_i))*W_f),int((y1/float(H_i))*H_f),int((x2/float(W_i))*W_f),int((y2/float(H_i))*H_f)
        x1c,y1c,x2c,y2c=max(0,x1f),max(0,y1f),min(W_f,x2f),min(H_f,y2f)
        if x1c<x2c and y1c<y2c: m[y1c:y2c,x1c:x2c]=1
        return m
    def _calc_global_weights(self, c: torch.Tensor, d_list: List[Dict], i_dims: Tuple[int,int]) -> torch.Tensor:
        B,K,Hf,Wf=c.shape; Hi,Wi=i_dims; gs=torch.zeros(B,K,device=c.device)
        for i,item_det_dict in enumerate(d_list):
            if not item_det_dict['boxes'].numel() or not item_det_dict['scores'].numel() or \
               (item_det_dict['scores'].numel()>0 and item_det_dict['scores'].max().item()==-float('inf')):
                gs[i]=torch.ones(K,device=c.device)/K; continue
            t_idx=item_det_dict['scores'].argmax(); t_box=item_det_dict['boxes'][t_idx]
            mask=self._box_to_mask(t_box,Hf,Wf,Hi,Wi)
            gs[i]=(c[i]*mask.unsqueeze(0)).mean(dim=[1,2])
        max_s=gs.max(dim=1,keepdim=True)[0]; exp_s=torch.exp(gs-max_s)
        return exp_s/(exp_s.sum(dim=1,keepdim=True)+self.eps)
    def __call__(self,c:torch.Tensor,d:List[Dict],i_dims:Tuple[int,int])->torch.Tensor:
        return self._calc_global_weights(c,d,i_dims)


# --- CRAFT_Concept_Explainer CLASS (Unchanged) ---
class CRAFT_Concept_Explainer:
    def __init__(self, num_concepts, feature_dim_for_nmf, nmf_max_iter=200, pgd_iter=50, pgd_lr=0.01, device='cuda'):
        self.K=num_concepts; self.feat_dim=feature_dim_for_nmf; self.nmf_iter=nmf_max_iter
        self.pgd_iter=pgd_iter; self.pgd_lr=pgd_lr; self.dev=device; self.W=None; self.sobol=SobolAnalyzer()
    def fit_nmf(self, feats_bchw: torch.Tensor):
        B,C,H,W=feats_bchw.shape; assert C==self.feat_dim, f"Feat dim mismatch. Expected {self.feat_dim}, got {C}"
        A=feats_bchw.permute(0,2,3,1).reshape(B*H*W,C).detach().cpu().numpy(); A=np.ascontiguousarray(A)
        nmf=NMF(n_components=self.K,max_iter=self.nmf_iter,random_state=42,init='nndsvda')
        try: nmf.fit(A); self.W=torch.from_numpy(nmf.components_.copy()).float().to(self.dev); print(f"CRAFT: NMF W learned: {self.W.shape}")
        except Exception as e: raise RuntimeError(f"NMF fit fail: {e}. A shape: {A.shape}")
    def get_U_spatial(self, feats_bchw: torch.Tensor):
        assert self.W is not None, "NMF W not learned."
        B,C,H,W=feats_bchw.shape; A=feats_bchw.permute(0,2,3,1).reshape(B*H*W,C)
        U=torch.rand(A.shape[0],self.K,device=self.dev,requires_grad=False)
        Wf=self.W.detach(); WTW,AWT=Wf@Wf.T,A@Wf.T
        for _ in range(self.pgd_iter): U=F.relu(U-self.pgd_lr*(U@WTW-AWT))
        return U.reshape(B,H,W,self.K).permute(0,3,1,2).detach()
    def get_sobol_scores(self, U_spatial_bkhw:torch.Tensor, dets_scaled:List[Dict], proc_img_dims:Tuple[int,int]):
        return self.sobol(U_spatial_bkhw, dets_scaled, proc_img_dims)

### START: NEW COMBINED VISUALIZATION FUNCTION ###
def visualize_combined_explanation(
    original_image_path: str,
    primary_detection: Dict,
    grad_cam_heatmap: torch.Tensor,
    spatial_concept_activations: torch.Tensor,
    global_k_sobol_scores: torch.Tensor,
    concept_names: Optional[List[str]] = None,
    num_top_concepts_to_show: Optional[int] = None,
    concept_display_threshold: float = 0.3,
    alpha_concept_overlay: float = 0.5
):
    """
    Creates a single, combined visualization showing the original image, the Grad-CAM heatmap,
    and the CRAFT concept overlay for the primary detection.

    Args:
        original_image_path (str): Path to the original image file.
        primary_detection (Dict): The dictionary for the primary detected object.
        grad_cam_heatmap (torch.Tensor): The pre-computed Grad-CAM heatmap (2D tensor).
        spatial_concept_activations (torch.Tensor): Tensor of shape (1, K, Hf, Wf) from CRAFT.
        global_k_sobol_scores (torch.Tensor): Tensor of shape (K,) with Sobol scores.
        concept_names (Optional[List[str]]): User-defined names for the K concepts.
        num_top_concepts_to_show (Optional[int]): If set, shows only top N concepts by Sobol score.
        concept_display_threshold (float): Threshold for displaying concept activations.
        alpha_concept_overlay (float): Transparency of the concept overlays.
    """
    # --- Setup: Load image and create figure ---
    try:
        orig_pil = Image.open(original_image_path).convert('RGB')
        orig_np = np.array(orig_pil)
    except Exception as e:
        print(f"Error in visualize_combined_explanation: Cannot load image {original_image_path}: {e}")
        return

    fig, axes = plt.subplots(1, 3, figsize=(24, 8))
    fig.suptitle("Combined Explanation for Primary Detection", fontsize=16)

    # --- Panel 1: Original Image with Bounding Box ---
    ax1 = axes[0]
    ax1.imshow(orig_np)
    ax1.set_title("1. Primary Detection", fontsize=12)
    ax1.axis('off')
    x1o, y1o, x2o, y2o = primary_detection['box_xyxy']
    rect1 = patches.Rectangle((x1o, y1o), x2o-x1o, y2o-y1o, fill=False, edgecolor='yellow', linewidth=2)
    ax1.add_patch(rect1)
    ax1.text(x1o, y1o - 10,
             f"{primary_detection['class_name']} ({primary_detection['score']:.2f})",
             color='yellow', fontsize=10, bbox=dict(facecolor='black', alpha=0.6))

    # --- Panel 2: Pixel-Level Explanation (Grad-CAM) ---
    ax2 = axes[1]
    heatmap_np = grad_cam_heatmap.numpy()
    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_np), cv2.COLORMAP_JET)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
    blend_img = cv2.addWeighted(heatmap_colored, 0.5, orig_np, 0.5, 0)
    ax2.imshow(blend_img)
    ax2.set_title("2. Pixel-Level Explanation (Grad-CAM)", fontsize=12)
    ax2.axis('off')
    rect2 = patches.Rectangle((x1o, y1o), x2o-x1o, y2o-y1o, fill=False, edgecolor='yellow', linewidth=2)
    ax2.add_patch(rect2)

    # --- Panel 3: Concept-Level Explanation (CRAFT/NMF) ---
    ax3 = axes[2]
    ax3.imshow(orig_np)
    ax3.set_title("3. Concept-Level Explanation (CRAFT)", fontsize=12)
    ax3.axis('off')

    concepts_feat_scale = spatial_concept_activations[0].detach().cpu()
    sobol_scores_np = global_k_sobol_scores.cpu().numpy()
    num_total_concepts = concepts_feat_scale.shape[0]

    if num_top_concepts_to_show is None or num_top_concepts_to_show >= num_total_concepts:
        indices_to_plot = np.argsort(sobol_scores_np)[::-1] # Sort by score
    else:
        indices_to_plot = np.argsort(sobol_scores_np)[::-1][:num_top_concepts_to_show]

    num_to_plot = len(indices_to_plot)
    cmap = plt.cm.get_cmap('tab10', max(10, num_to_plot)) if num_to_plot <= 10 else plt.cm.get_cmap('gist_rainbow', num_to_plot)
    legend_handles = []

    for i, nmf_concept_idx in enumerate(indices_to_plot):
        concept_map = concepts_feat_scale[nmf_concept_idx].numpy()
        cm_min, cm_max = concept_map.min(), concept_map.max()
        if cm_max - cm_min < 1e-8: continue
        norm_concept_map = (concept_map - cm_min) / (cm_max - cm_min)
        norm_concept_map[norm_concept_map < concept_display_threshold] = 0
        if np.any(norm_concept_map > 0):
            concept_map_pil = Image.fromarray((norm_concept_map * 255).astype(np.uint8)).resize(orig_pil.size, Image.LANCZOS)
            concept_color_rgba = cmap(i / float(max(1, num_to_plot - 1)))
            custom_cmap = ListedColormap([(0,0,0,0), concept_color_rgba])
            ax3.imshow(np.array(concept_map_pil), cmap=custom_cmap, alpha=alpha_concept_overlay, vmin=0, vmax=255)
            c_name = concept_names[nmf_concept_idx] if concept_names and nmf_concept_idx < len(concept_names) else f"Concept {nmf_concept_idx + 1}"
            sobol_val = sobol_scores_np[nmf_concept_idx]
            legend_handles.append(patches.Patch(color=concept_color_rgba, label=f"{c_name} (Sobol: {sobol_val:.3f})"))

    if legend_handles:
        fig.legend(handles=legend_handles, loc='upper right', bbox_to_anchor=(0.99, 0.85),
                   title="Concepts & Importance", fontsize=9)

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()
### END: NEW COMBINED VISUALIZATION FUNCTION ###

def apply_object_mask(heatmap: np.ndarray, bbox: list, threshold: float = 0.5) -> np.ndarray:
    x1, y1, x2, y2 = map(int, bbox)
    masked_heatmap = np.zeros_like(heatmap)
    cropped_heatmap = heatmap[y1:y2, x1:x2]
    cropped_heatmap[cropped_heatmap < threshold] = 0
    masked_heatmap[y1:y2, x1:x2] = cropped_heatmap
    return masked_heatmap

def visualize_explanation(orig_img_path: str,
                          conv_features: torch.Tensor,
                          detection: Dict,
                          output_logits: torch.Tensor,
                          class_idx: int,
                          threshold: float = 0.5) -> None:
    image = Image.open(orig_img_path).convert('RGB')
    img_array = np.array(image)
    h, w = img_array.shape[:2]

    conv_features.requires_grad_(True)
    conv_features.retain_grad()
    score_tensor = output_logits[class_idx, detection['label_idx']]
    score_tensor.backward(retain_graph=True)
    gradients = conv_features.grad
    if gradients is None:
        raise RuntimeError("conv_features.grad is None.")

    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])
    weighted_features = conv_features[0] * pooled_gradients.view(-1, 1, 1)
    heatmap = torch.mean(weighted_features, dim=0).detach().cpu().numpy()
    heatmap = np.maximum(heatmap, 0)
    heatmap /= np.max(heatmap) + 1e-8
    heatmap = cv2.resize(heatmap, (w, h))
    heatmap_masked = apply_object_mask(heatmap, detection['box_xyxy'], threshold=threshold)
    heatmap_uint8 = np.uint8(255 * heatmap_masked)
    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)
    blend = np.clip(img_array * 0.5 + heatmap_color * 0.5, 0, 255).astype(np.uint8)

    x1, y1, x2, y2 = map(int, detection['box_xyxy'])
    cv2.rectangle(blend, (x1, y1), (x2, y2), (0, 255, 0), 2)
    label = detection.get('class_name', 'object')
    sobol_score = detection.get('sobol_score', None)
    text = f"{label}"
    if sobol_score is not None:
        text += f" (Sobol: {sobol_score:.2f})"
    cv2.putText(blend, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

    fig, axs = plt.subplots(1, 2, figsize=(16, 8))
    axs[0].imshow(img_array); axs[0].set_title("Original Image"); axs[0].axis('off')
    axs[1].imshow(blend); axs[1].set_title("Object-Focused Grad-CAM with Box & Label"); axs[1].axis('off')
    plt.tight_layout(); plt.show()

def visualize_multiple_detections(
    orig_img_path: str,
    conv_features: torch.Tensor,
    output_logits: torch.Tensor,
    detections: List[Dict],
    threshold: float = 0.5
):
    for det in detections:
        visualize_explanation(orig_img_path, conv_features, det, output_logits, class_idx=det['query_index'], threshold=threshold)

def visualize_spatial_nmf_concepts(
    original_image_path: str,
    spatial_concept_activations: torch.Tensor,
    global_k_sobol_scores: torch.Tensor,
    concept_names: Optional[List[str]] = None,
    num_top_concepts_to_show: Optional[int] = None,
    concept_display_threshold: float = 0.3,
    alpha_concept_overlay: float = 0.5,
    primary_detection: Optional[Dict] = None
):
    try:
        orig_pil = Image.open(original_image_path).convert('RGB')
    except Exception as e:
        print(f"Error in visualize_spatial_nmf_concepts: Cannot load image {original_image_path}: {e}")
        return

    fig, ax = plt.subplots(1, 1, figsize=(12, 9))
    ax.imshow(orig_pil); ax.axis('off')
    if primary_detection:
        x1o, y1o, x2o, y2o = primary_detection['box_xyxy']
        rect = plt.Rectangle((x1o, y1o), x2o-x1o, y2o-y1o, fill=False, ec='yellow', lw=2.5)
        ax.add_patch(rect)
        ax.text(x1o, y1o-10, f"{primary_detection['class_name']} ({primary_detection['score']:.2f})",
                color='yellow', fontsize=10, bbox=dict(fc='black', alpha=0.6))

    concepts_feat_scale = spatial_concept_activations[0].detach().cpu()
    sobol_scores_np = global_k_sobol_scores.cpu().numpy()
    num_total_concepts = concepts_feat_scale.shape[0]

    if num_top_concepts_to_show is None or num_top_concepts_to_show >= num_total_concepts:
        indices_to_plot = np.arange(num_total_concepts)
        plot_title_suffix = f"All {num_total_concepts}"
    else:
        indices_to_plot = np.argsort(sobol_scores_np)[::-1][:num_top_concepts_to_show]
        plot_title_suffix = f"Top {num_top_concepts_to_show} by Sobol Score"
    ax.set_title(f'Spatial NMF Concept Activations ({plot_title_suffix})')

    num_to_plot = len(indices_to_plot)
    cmap = plt.cm.get_cmap('tab10', max(10, num_to_plot)) if num_to_plot <= 10 else plt.cm.get_cmap('gist_rainbow', num_to_plot)
    legend_handles = []

    for i, nmf_concept_idx in enumerate(indices_to_plot):
        concept_map = concepts_feat_scale[nmf_concept_idx].numpy()
        cm_min, cm_max = concept_map.min(), concept_map.max()
        if cm_max - cm_min > 1e-8:
            norm_concept_map = (concept_map - cm_min) / (cm_max - cm_min)
        else: continue
        norm_concept_map[norm_concept_map < concept_display_threshold] = 0
        if np.any(norm_concept_map > 0):
            concept_map_pil = Image.fromarray((norm_concept_map * 255).astype(np.uint8)).resize(orig_pil.size, Image.LANCZOS)
            concept_color_rgba = cmap(i / float(max(1, num_to_plot - 1)))
            custom_cmap = ListedColormap([(0,0,0,0), concept_color_rgba])
            ax.imshow(np.array(concept_map_pil), cmap=custom_cmap, alpha=alpha_concept_overlay, vmin=0, vmax=255)
            c_name = concept_names[nmf_concept_idx] if concept_names and nmf_concept_idx < len(concept_names) else f"NMF Concept {nmf_concept_idx + 1}"
            sobol_val = sobol_scores_np[nmf_concept_idx]
            legend_handles.append(patches.Patch(color=concept_color_rgba, label=f"{c_name} (Sobol: {sobol_val:.2f})"))

    if legend_handles:
        ax.legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(1.01, 1), title="Concepts & Sobol Scores")
        plt.tight_layout(rect=[0, 0, 0.82, 1])
    else: plt.tight_layout()
    plt.show()

def scale_detections_for_sobol(
    detections_orig_scale: List[Dict],
    original_pil_size: Tuple[int, int],
    processed_img_dims: Tuple[int, int],
    device: str
) -> List[Dict]:
    if not detections_orig_scale:
        return [{'boxes': torch.empty(0, 4, device=device), 'scores': torch.empty(0, device=device),
                 'labels': torch.empty(0, dtype=torch.long, device=device)}]
    scaled_boxes, scores, labels = [], [], []
    orig_w, orig_h = original_pil_size
    proc_h, proc_w = processed_img_dims
    for det in detections_orig_scale:
        x1o, y1o, x2o, y2o = det['box_xyxy']
        x1p = (x1o / orig_w) * proc_w; y1p = (y1o / orig_h) * proc_h
        x2p = (x2o / orig_w) * proc_w; y2p = (y2o / orig_h) * proc_h
        scaled_boxes.append([x1p, y1p, x2p, y2p])
        scores.append(det['score']); labels.append(det['label_idx'])
    return [{'boxes': torch.tensor(scaled_boxes, dtype=torch.float32, device=device),
             'scores': torch.tensor(scores, dtype=torch.float32, device=device),
             'labels': torch.tensor(labels, dtype=torch.int64, device=device)}]



# --- Main Execution ---
if __name__ == '__main__':
    IMAGE_PATH = '/content/test5.jpg'
    if not os.path.exists(IMAGE_PATH):
        try:
            print(f"WARN: '{IMAGE_PATH}' not found. Downloading a demo image.")
            url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
            response = requests.get(url, stream=True); response.raise_for_status()
            with open(IMAGE_PATH, 'wb') as f: f.write(response.content)
            print(f"Demo image saved to '{IMAGE_PATH}'")
        except Exception as e:
            print(f"Demo image download failed: {e}. Exiting."); exit()

    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {DEVICE}")

    # --- 1. ODAM: Detection & Feature Extraction ---
    print("\n--- 1. ODAM: Initializing and processing image ---")
    odam_proc = ODAM_Processor(device=DEVICE)
    pil_img_orig = Image.open(IMAGE_PATH).convert('RGB')
    dets_all, feats_craft_bchw, raw_detr_out, _, proc_dims_feats_img = \
        odam_proc.get_detections_features_and_input_tensor(pil_img_orig, conf_thresh=0.7)
    if not dets_all: print("ODAM: No detections found. Exiting."); exit()
    print(f"ODAM: Found {len(dets_all)} objects.")
    primary_det_obj = sorted(dets_all, key=lambda x: x['score'], reverse=True)[0]
    print(f"ODAM: Selected primary detection: '{primary_det_obj['class_name']}' (score: {primary_det_obj['score']:.2f})")

    # --- 2. ODAM: Gradient-based Heatmap Generation for the PRIMARY object ---
    print("\n--- 2. ODAM: Generating gradient explanation heatmap for primary object---")
    # This heatmap is specifically for the combined visualization
    explanation_heatmap = odam_proc.generate_gradient_explanation_heatmap(
        raw_detr_out, primary_det_obj, pil_img_orig.size)

    # --- 3. CRAFT: Concept Learning & Sobol Scores ---
    NUM_NMF_CONCEPTS = 5
    C_feat = feats_craft_bchw.shape[1]
    print(f"\n--- 3. CRAFT: Initializing (Concepts: {NUM_NMF_CONCEPTS}) ---")
    craft_expl = CRAFT_Concept_Explainer(NUM_NMF_CONCEPTS, C_feat, device=DEVICE)
    print("CRAFT: Fitting NMF to learn concept dictionary W...")
    craft_expl.fit_nmf(feats_craft_bchw)
    print("CRAFT: Computing spatial concept activation maps U...")
    U_spatial = craft_expl.get_U_spatial(feats_craft_bchw)
    print("CRAFT: Scaling detections and calculating Sobol scores...")
    dets_sobol_scaled = scale_detections_for_sobol(dets_all, pil_img_orig.size, proc_dims_feats_img, DEVICE)
    k_sobol_w_batch = craft_expl.get_sobol_scores(U_spatial, dets_sobol_scaled, proc_dims_feats_img)
    k_sobol_w_img = k_sobol_w_batch[0]

    # --- 4. Individual Visualizations ---
    user_concept_names = [f"Concept {i+1}" for i in range(NUM_NMF_CONCEPTS)]
    print("\n--- 4. Generating individual visualizations ---")
    print("\n--- Vis 1: Grad-CAM for ALL Detections ---")
    raw_detr_out['conv_features'].requires_grad_(True)
    visualize_multiple_detections(IMAGE_PATH, raw_detr_out['conv_features'], raw_detr_out['pred_logits'][0], dets_all)

    print("\n--- Vis 2: Spatial NMF Concepts (Sorted by Importance) ---")
    visualize_spatial_nmf_concepts(IMAGE_PATH, U_spatial, k_sobol_w_img,
        concept_names=user_concept_names, num_top_concepts_to_show=NUM_NMF_CONCEPTS,
        primary_detection=primary_det_obj)

    # --- 5. FINAL COMBINED VISUALIZATION ---
    print("\n--- Vis 3: Combined Explanation for Primary Detection ---")
    visualize_combined_explanation(
        original_image_path=IMAGE_PATH,
        primary_detection=primary_det_obj,
        grad_cam_heatmap=explanation_heatmap, # Use the heatmap calculated in step 2
        spatial_concept_activations=U_spatial,
        global_k_sobol_scores=k_sobol_w_img,
        concept_names=user_concept_names,
        num_top_concepts_to_show=NUM_NMF_CONCEPTS
    )
    print("\n--- Done ---")